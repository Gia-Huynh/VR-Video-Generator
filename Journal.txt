Maria Nagai 0.04 -0.2 (Or 0.02?).mkv is the holy grail, jesus.
So now I’m testing the same config (0.04, -0.02) on other videos.

Found a way to optimize speed, let the gpu handle np.roll (torch.roll)
torch.roll (img_gpu, shifts=i, dims=1).cpu().numpy()

Tôi ngày 10/10: Tối nay chạy video marianagai concutoiphatno.mkv, và video worstorbest (KILL ME GPUROLL.mkv), để test 0.11 -0.33.
Note: Chạy lại line profiler để xem cái nào là bottleneck, nhưng ưu tiên thấp.
Ưu tiên cao nhất là get a working model, then BUILD THE ACTUAL GOOD UI FOR IT.


10/10:
(np.sum (cv2.absdiff (cv2.stackBlur(raw_img, (3, 3)), cv2.stackBlur(self.last_frame, (3, 3)))) < 2000000) IS NOT AGNOSTIC TO IMAGE SIZE, FUCK ME, need to normalize
Currently our worst bottleneck is waiting for gpu to predict the image.
Hiện tại ratio cho clip maria nagai là 86p video tốn 460p chạy, gpu unoptimized usage.

11/10:
Đã optimize left_side_sbs sang xài gpu, ratio từ 5x xuống 3x, khá oke
User interface next.
Không thặc sự là 3x tí nào cả, vẫn đâu đó 4x-5x vì gpu giờ làm nhiều operation quá, nó khiến  prediction chậm đi, và chuyển dữ liệu từ gpu sang numpy và ngược lại lâu vailoz.
nên modify dpt.py tạo thêm hàm infer_image_gpu để đỡ dịch file.
TỐC ĐỘ NHANH VÃI LỒN ÚI CHỜI ƠI.

ĐỜ CỜ MỜ Queue có vấn đề, item bỏ vào khác item rút ra, chắc do mình khôn lỏi, thử đổi lại làm chính quy xem thế nào.
Làm chính quy xong khá ổn, optimize cái edge_fill kernel để khỏi tạo lại, rồi làm queue riêng cho từng worker, thêm notify queue cho gpu_worker nữa.
Từ 5 tỉ xuống 2 tỉ 9.

Bước tiếp theo: Tạm comment cái temporal dampening xem có nhanh hơn nữa không, mục tiêu là 2x.
Anyway, bước ăn time nhất hiện tại (1 tỷ 2) là torch.nonzero, mình đang để output nó vào biến sẵn để giảm thời gian khởi tạo memory, nhưng...
    torch.nonzero(bin_mask, as_tuple=False, out = self.non_zero_indices) RuntimeError: CUDA error: device-side assert triggered
	(nguyên nhân do gán out= thì torch nó chép đại vào mà không quan tâm biến có đủ bộ nhớ chứa khum)

Tối 11/10: Đã chính thức chạm được ngưỡng 2x thời gian, maria nagai 86 phút giờ chạy trong 216 phút (with temporal dampening of course).
Sáng 12/10: Tình hình là mấy thằng fill edge ở sau khúc loop nó vừa aggresive ở cái chọn pixel nhưng lại un-aggresive ở cái blur intensity, nên giờ giảm edge_fill conv2d lại, giảm kernel_size lại, và tăng sigma lên
REMOVE Edge fill luôn, vì giờ chạy nhanh quá nên offset_list mình không cần để step=2 nữa, nên không cần compromise

vits chạy được 2 model + 8-10 worker.
vitb chạy được 1 model là nó ăn gần 4gb vram rồi, 2 model chắc nổ cu.
vitb 1 model + 6 worker = 7-8gb vram.
chắc được max 8 worker
nvm tràn mẹ nó ram

Nua dem 15/10: Nếu để offset_fg và offset_bg lớn và với wrong config thì tự dưng bị tràn full 100% vram gpu kẹt cứng ngắc, used to happened on some night run. 
Config dễ tràn nhất là 0.06 fg và -0.1 bg với đúng 8 worker và 2 gpu worker, nhưng ngược lại nếu giảm worker đi một chút (hình như 6-1) thì chỉ ăn có 3-4 gb vram.
Dùng pystuck để nhảy process check thì từ 0 tới 7 đều không báo ăn vram pytorch.
Nhưng tất cả đều bị kẹt ở     result_img[result_zero_mask] = (gaussian_blur(result_img.permute (2,0,1),    HOẶC LÀ Torch.nonzero() gì đó.
để nhảy vào gpu process xem nó ăn cái gì,  queue_idx = notify_queue_list.get() (là gpu worker idling around waiting for mấy cái kia finish stuff).

Nửa đêm 16/10: Đã sửa xong bằng cách aggresively thêm torch.empty_cache() và gc.collect() trong gpu producer process, 
có lẽ vì reference tới result_img đã bị hủy bỏ nhưng torch không biết để empty cache mà vẫn giữ nó lại, pilling up và tràn vram.
Giờ thì ram siêu stable. Nhung gpu usage and cpu usage low AF

Vấn đề ram gpu đã sửa xong bằng cách tạo danh sách cố định depth-frame cho từng worker.
Giờ xử cái vụ high offset.

19/10: [High offset/Depth issue]: Có thể do mình del và overwrite lên depth cũ (có lẽ) trên inference_worker nên bên kia chạy bị lỗi depth, hiện tại sẽ lưu lại 2 depth luôn, 1 cái đã pass vào queue, 1 cái vừa predict nhét thêm vào queue, mỗi lần nhận vào ảnh predict mới thì xóa 1 cái cũ nhất và chèn cái đã predict mới append vào.
KHÔNG, là do multiprocess Queue passing GPU Tensor is retarded.

Về vấn đề vram vitb.
vitb Sleep on beginning of left_side_sbs ăn tổng 3.3gb vram dedicated
vitb Sleep on ending of left_side_sbs tràn mẹ vram
vits Sleep on beginning of left_side_sbs ăn tổng 2.8gb vram dedicated
vits Sleep on ending of left_side_sbs ăn tổng 7.4gb vram

Hóa ra output của vitb và vits khác nhau hoàn toàn về scale, dẫn tới get_cutoff output khác nhau hẳn.

Owl3D chị gái vú to dịch foreground qua trái 4 pixel, không dịch background.
Ớ HÓA RA MÌNH LÀM NGƯỢC À?
Mình làm chị máy bay vú to 0.01 thì cánh tay foreground chỉ dịch có 3 pixel.
nên test thử với 0.02
và thằng owl mặc định nó không dịch background

22/10: Somehow chọn flip L-R thì lại ra cực ổn áp với 0.025 -0.004, nổ cu, nên mình phải flip cái left_side_sbs trở lại.

28/10: MÌNH ĐANG HARD CODED PATH TO MODEL.
Thêm lỗi Triton not available.
Thêm filedialog phải check điều kiện path to pictures folder is in OneDrive