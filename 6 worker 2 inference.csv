Function,Line #,Line Content,Hits,Total Time (µs),Per Hit (µs)
inference_worker_backup,87,"    redirrect_stdout(DebugDir + f""inference_worker_{os.getpid()}.txt"")",1,"1,751",1751
inference_worker_backup,88,"    print_flush (encoder, encoder_path, DEVICE)",1,899,899
inference_worker_backup,89,"    print_flush (""Torch model loading into device name: "", torch.cuda.get_device_name(DEVICE))",1,968,968
inference_worker_backup,90,"    model = load_model(encoder, encoder_path, DEVICE)",1,"7,498,871",7498871
inference_worker_backup,91,"    print_flush (""Model loaded, trying to infer an image..."")",1,666,666
inference_worker_backup,92,"    model.infer_image (np.zeros((1080, 1920, 3), dtype = np.uint8))",1,"3,925,028",3925028
inference_worker_backup,93,    torch.cuda.empty_cache(),1,"36,493",36493
inference_worker_backup,94,"    print_flush (""Model loaded"")",1,607,607
inference_worker_backup,95,    while True:,454,"2,840",6.26
inference_worker_backup,96,        queue_idx = notify_queue_list.get(),454,"47,590,273",104824.39
inference_worker_backup,97,        if queue_idx is None:,454,"2,135",4.7
inference_worker_backup,98,            break,1,4,4
inference_worker_backup,99,        task = in_queue_list[queue_idx[0]].get(),453,"23,907,217",52775.31
inference_worker_backup,100,        if task is None:,453,"3,613",7.98
inference_worker_backup,102,        img = task[0],453,"922,072",2035.48
inference_worker_backup,103,"        with torch.no_grad(), autocast(device_type=DEVICE.type, dtype=torch.float16):",906,"1,634,897",1804.52
inference_worker_backup,104,            result = model.infer_image_gpu(img),453,"529,175,078",1168156.91
inference_worker_backup,105,        out_queue_list[queue_idx[0]].put(result),453,"223,229",492.78
