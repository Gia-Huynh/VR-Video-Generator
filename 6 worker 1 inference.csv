Function,Line #,Line Content,Hits,Total Time (µs),Per Hit (µs)
inference_worker_backup,87,"    redirrect_stdout(DebugDir + f""inference_worker_{os.getpid()}.txt"")",1,"1,792",1792
inference_worker_backup,88,"    print_flush (encoder, encoder_path, DEVICE)",1,"1,111",1111
inference_worker_backup,89,"    print_flush (""Torch model loading into device name: "", torch.cuda.get_device_name(DEVICE))",1,994,994
inference_worker_backup,90,"    model = load_model(encoder, encoder_path, DEVICE)",1,"6,228,884",6228884
inference_worker_backup,91,"    print_flush (""Model loaded, trying to infer an image..."")",1,615,615
inference_worker_backup,92,"    model.infer_image (np.zeros((1080, 1920, 3), dtype = np.uint8))",1,"4,610,392",4610392
inference_worker_backup,93,    torch.cuda.empty_cache(),1,"38,554",38554
inference_worker_backup,94,"    print_flush (""Model loaded"")",1,902,902
inference_worker_backup,95,    while True:,907,"4,458",4.92
inference_worker_backup,96,        queue_idx = notify_queue_list.get(),907,"17,796,436",19621.21
inference_worker_backup,97,        if queue_idx is None:,907,"4,120",4.54
inference_worker_backup,98,            break,1,4,4
inference_worker_backup,99,        task = in_queue_list[queue_idx[0]].get(),906,"36,303,445",40070.03
inference_worker_backup,100,        if task is None:,906,"6,349",7.01
inference_worker_backup,102,        img = task[0],906,"1,656,158",1827.99
inference_worker_backup,103,"        with torch.no_grad(), autocast(device_type=DEVICE.type, dtype=torch.float16):",1812,"2,613,331",1442.24
inference_worker_backup,104,            result = model.infer_image_gpu(img),906,"635,834,920",701804.55
inference_worker_backup,105,        out_queue_list[queue_idx[0]].put(result),906,"444,039",490.11
